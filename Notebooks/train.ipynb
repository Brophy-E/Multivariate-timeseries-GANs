{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "o0U70ZKdGG-u",
        "2-TFlGkJGRwk",
        "aVNp-z6p9YPA",
        "CE5EtmZiHaJ7",
        "IR59cDIu_TTb",
        "ODyxSKAa9bgH",
        "yZuSPFqqCB74",
        "nB-LMbWvPhgK",
        "idsaZ-_l7Afk",
        "c2KbZAn--c8x",
        "oQrQ_xQcPqCF",
        "ocke51Wh_AuT",
        "nqkMV5CVPKtf",
        "0tB00unuxqzM",
        "cCQjK7_9_Gsm",
        "ayJkFQOud5Ef",
        "QWG8tb9QOTWs",
        "ooFkDiwNPsti"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMRjwtx68_b4"
      },
      "source": [
        "# Multivariate Time Series Generation - Train the model\n",
        "\n",
        "This Notebook demonstrates the ability of our Model to generate dependent multi-channel physiological signals\n",
        "\n",
        "This repository is an artifact for the paper under review \"Multivariate Generative Adversarial Networks and their Loss Functions for Synthesis of Multichannel ECGs\" submitted to IEEE Pattern Recognition and Machine Intelligence 2020.\n",
        "\n",
        "The code has been supplied as Jupyter Notebooks and set up to run in Google Colaboratory. The dataset used is open source and freely available from PhysioNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0U70ZKdGG-u"
      },
      "source": [
        "## If Using Google Colabs\n",
        "\n",
        "Mount your drive if you are running this on Colabs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf-HLHeoGGRk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51711789-e9aa-454f-8e59-eca211cb41fa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-TFlGkJGRwk"
      },
      "source": [
        "## Directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz__mIRXEnHo"
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/drive/My Drive/MV_GAN_Journal/Multivariate_time_series_gen/')\n",
        "\n",
        "# Make Directories to store Training Results\n",
        "training_dir = '/content/drive/My Drive/MV_GAN_Journal/Multivariate_time_series_gen/Results/Loss-SenseGAN_NSR'\n",
        "if not os.path.exists(training_dir):\n",
        "  os.mkdir(training_dir)\n",
        "\n",
        "# Directories for different MBD Layers (losses and models) \n",
        "# -- MBD not used in this work but can be implemented\n",
        "minibatch_layer = [0, 3, 5, 8, 10]\n",
        "for i in minibatch_layer:\n",
        "  mbd_dir = (training_dir+'/MBD_'+str(i))\n",
        "  if not os.path.exists(mbd_dir):\n",
        "    os.mkdir(mbd_dir)\n",
        "    os.mkdir(mbd_dir+'/gen')\n",
        "    os.mkdir(mbd_dir+'/disc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhLqYerAvm7d"
      },
      "source": [
        "Data and Save Directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj_ci4GMvmLr"
      },
      "source": [
        "savepath = training_dir\n",
        "datapath = './Data/'\n",
        "\n",
        "# Choose which dataset you want here.\n",
        "#datafile = 'ECG_Arr.pt' # ECG Arrhythmia Dataset\n",
        "datafile = 'ecg_mit_nsnr.pt' # ECG Normal Sinus Rhythm Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVNp-z6p9YPA"
      },
      "source": [
        "## Import necessary dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3jKSp7k89m4"
      },
      "source": [
        "import json as js\n",
        "import pickle\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.autograd.variable import Variable\n",
        "import torch.autograd as autograd\n",
        "\n",
        "from model import Generator, Discriminator\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMX0rJf8qKNg",
        "outputId": "27c4e23a-6877-4f31-9f31-643936146134"
      },
      "source": [
        "!pip install fastdtw\n",
        "\n",
        "from scipy.spatial.distance import sqeuclidean\n",
        "from fastdtw import fastdtw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastdtw in /usr/local/lib/python3.6/dist-packages (0.3.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastdtw) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHcf_EfVfx-H"
      },
      "source": [
        " The R MVDTW package is used here for speed of computation. The Dependent MVDTW is implemented in this Notebook but takes a long time to execute. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4stnKqmF89lB"
      },
      "source": [
        "import rpy2.robjects.numpy2ri\n",
        "from rpy2.robjects.packages import importr\n",
        "rpy2.robjects.numpy2ri.activate()\n",
        "import rpy2.robjects as robj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBaydnQv8_zx",
        "outputId": "c4e22454-6bdf-4306-af82-5c4b4f869e64"
      },
      "source": [
        "%load_ext rpy2.ipython"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:14: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
            "  from pandas.core.index import Index as PandasIndex\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:34: UserWarning: pandas >= 1.0 is not supported.\n",
            "  warnings.warn('pandas >= 1.0 is not supported.')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ-f-RSt9Abv"
      },
      "source": [
        "%%R\n",
        "install.packages(\"dtw\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_7bvw5-9DwZ"
      },
      "source": [
        "# Set up our R namespaces\n",
        "R = rpy2.robjects.r\n",
        "DTW = importr('dtw')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE5EtmZiHaJ7"
      },
      "source": [
        "## GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uShHKR3vElDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f25c667-c034-414e-a7c8-804178c4a8fd"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  cuda = True\n",
        "  print('Using: ' +str(torch.cuda.get_device_name(device)))\n",
        "else:\n",
        "  print('Using: CPU')\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR59cDIu_TTb"
      },
      "source": [
        "## Initialise Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqkAp_Fg-HUH"
      },
      "source": [
        "\"\"\"Initialising Parameters\"\"\"\n",
        "def init_params(data='ecg_mit_nsnr.pt'):\n",
        "  # Loaded from Preprocessing step\n",
        "  datafile = data\n",
        "\n",
        "  # Number of features\n",
        "  seq_len = 500 \n",
        "  batch_size = 50 \n",
        "\n",
        "  # Params for the generator\n",
        "  hidden_nodes_g = 50\n",
        "  layers = 2\n",
        "  tanh_layer = False\n",
        "\n",
        "  # No. of training rounds per epoch\n",
        "  D_rounds = 3\n",
        "  G_rounds = 1\n",
        "  num_epochs = 50\n",
        "  learning_rate = 0.0002\n",
        "\n",
        "  # Loss weight for gradient penalty\n",
        "  lambda_gp = 10\n",
        "\n",
        "  # Params for the Discriminator\n",
        "  minibatch_layer = [0, 3, 5, 8, 10]\n",
        "  minibatch_normal_init = False\n",
        "  num_cvs = 2\n",
        "  cv1_out= 10\n",
        "  cv1_k = 3\n",
        "  cv1_s = 1\n",
        "  p1_k = 3\n",
        "  p1_s = 2\n",
        "  cv2_out = 10\n",
        "  cv2_k = 3\n",
        "  cv2_s = 1\n",
        "  p2_k = 3\n",
        "  p2_s = 2\n",
        "\n",
        "  # Create Dictionary - for re-use\n",
        "  params = {\n",
        "      'data' : datafile,\n",
        "      'seq_len' : seq_len,\n",
        "      'batch_size' : batch_size,\n",
        "      'hidden_nodes_g': hidden_nodes_g,\n",
        "      'layers':layers,\n",
        "      'tanh_layer':tanh_layer,\n",
        "      'D_rounds' : D_rounds,\n",
        "      'G_rounds': G_rounds,\n",
        "      'epochs': num_epochs,\n",
        "      'learning_rate' : learning_rate,\n",
        "      'lambda_gp' : lambda_gp,\n",
        "      'minibatch_layer' : minibatch_layer,\n",
        "      'minibatch_normal_init' : minibatch_normal_init,\n",
        "      'num_cvs' : num_cvs,\n",
        "      'cv1_out' : cv1_out,\n",
        "      'cv1_k' : cv1_k,\n",
        "      'cv1_s' : cv1_s,\n",
        "      'p1_k' : p1_k,\n",
        "      'p1_s' : p1_s,\n",
        "      'cv2_out' : cv2_out,\n",
        "      'cv2_k' : cv2_k,\n",
        "      'cv2_s' : cv2_s,\n",
        "      'p2_k' : p2_k,\n",
        "      'p2_s' : p2_s   \n",
        "  }\n",
        "\n",
        "  return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODyxSKAa9bgH"
      },
      "source": [
        "## Function Definitions\n",
        "\n",
        "\n",
        "*   Load Data\n",
        "*   Noise Function\n",
        "*   Gradient Penalty\n",
        "*   Evaluation\n",
        "*   Save Params to JSON\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJXtX0AU9sUj"
      },
      "source": [
        "def load_data(filename, batch_size):\n",
        "    mv_data = torch.load(filename)\n",
        "    if len(mv_data[0,:,0]) == 501:\n",
        "        mv_data = mv_data[:, :-1, :] \n",
        "    data_loader = torch.utils.data.DataLoader(mv_data, batch_size=batch_size)\n",
        "    num_batches = len(data_loader)\n",
        "    \n",
        "    return data_loader, num_batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe7j-_rTAYJD"
      },
      "source": [
        "def noise(batch_size, features):\n",
        "    noise_vec = torch.randn(2, batch_size, features).to(device)\n",
        "    \n",
        "    return noise_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsuALKgW_dq7"
      },
      "source": [
        "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
        "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
        "    # Random weight term for interpolation between real and fake samples\n",
        "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
        "    # Get random interpolation between real and fake samples\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "    d_interpolates = D(interpolates)\n",
        "    fake = Variable(Tensor(real_samples.shape[0], 2).fill_(1.0), requires_grad=False)\n",
        "    # Get gradient w.r.t. interpolates\n",
        "    gradients = autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z86p-6q2_e3w"
      },
      "source": [
        "def save_params(params, filename):\n",
        "    json = js.dumps(params)\n",
        "    f = open(filename+'/parameters.json','w')\n",
        "    f.write(json)\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZuSPFqqCB74"
      },
      "source": [
        "### Load Model\n",
        "\n",
        "*Load and Initialise*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_73yF2xCFph"
      },
      "source": [
        "def load_model(params, minibatch_layer):\n",
        "    \n",
        "    generator = Generator(params['seq_len'], params['batch_size'], hidden_dim = params['hidden_nodes_g'], tanh_output = params['tanh_layer']).to(device)\n",
        "    discriminator = Discriminator(params['seq_len'], in_channels=2,\n",
        "                          cv1_k=3, cv1_s=1, p1_k=3, p1_s=1,\n",
        "                          cv2_k=3, cv2_s=1, p2_k=3, p2_s=2,\n",
        "                          cv3_k=3, cv3_s=2, p3_k=3, p3_s=2,\n",
        "                          cv4_k=5, cv4_s=2, p4_k=5, p4_s=2, \n",
        "                          minibatch_layer = minibatch_layer, minibatch_init = params['minibatch_normal_init']).to(device)\n",
        "\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    \n",
        "    # Optimizers\n",
        "    g_optimizer = torch.optim.RMSprop(generator.parameters(), lr = params['learning_rate'])\n",
        "    d_optimizer = torch.optim.RMSprop(discriminator.parameters(), lr = params['learning_rate'])\n",
        "    \n",
        "    return generator, discriminator, g_optimizer, d_optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB-LMbWvPhgK"
      },
      "source": [
        "## DTW Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idsaZ-_l7Afk"
      },
      "source": [
        "###R_DTW Package\n",
        "This package is provided and implemented here only for speed of computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbdi-p8T7FE2"
      },
      "source": [
        "def R_DTW(real, fake):\n",
        "  dtw_dist = []\n",
        "\n",
        "  for i in range(len(real)):\n",
        "    #for j in range(len(fake)):\n",
        "      X = real[i,:,:,0].detach().cpu().numpy()\n",
        "      Y = fake[i,:,:,0].detach().cpu().numpy()\n",
        "\n",
        "      template = X.transpose()\n",
        "      rt,ct = template.shape\n",
        "      query = Y.transpose()\n",
        "      rq,cq = query.shape\n",
        "\n",
        "      #converting numpy matrices to R matrices\n",
        "      templateR=R.matrix(template,nrow=rt,ncol=ct)\n",
        "      queryR=R.matrix(query,nrow=rq,ncol=cq)\n",
        "\n",
        "      # Calculate the alignment vector and corresponding distance\n",
        "      alignment = R.dtw(templateR,queryR,keep=True, step_pattern=R.rabinerJuangStepPattern(4,\"c\"),open_begin=True,open_end=True)\n",
        "\n",
        "      dist = alignment.rx('distance')[0][0]\n",
        "      dtw_dist.append(dist)\n",
        "\n",
        "  return dtw_dist\n",
        "\n",
        "def FAST_DTW(real, fake):\n",
        "  dtw_dist = []\n",
        "  for i in range(50):\n",
        "    #for j in range(len(fake)):\n",
        "    x = real[i,:,:,0]\n",
        "    y = fake[i,:,:,0]\n",
        "\n",
        "    d0, _ = fastdtw(x[0].view(1,-1).detach().cpu().numpy(), y[0].view(1,-1).detach().cpu().numpy(), dist=sqeuclidean)\n",
        "    d1, _ = fastdtw(x[1].view(1,-1).detach().cpu().numpy(), y[1].view(1,-1).detach().cpu().numpy(), dist=sqeuclidean)\n",
        "    dtw_dist.append([d0,d1])\n",
        "\n",
        "  return dtw_dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2KbZAn--c8x"
      },
      "source": [
        "### DTW Programatically"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr8jqwZf-dK8"
      },
      "source": [
        "def distance_matrix(Q, C):\n",
        "    matrix = np.ones((len(C), len(Q)))\n",
        "    for i in range(len(C)):\n",
        "        for j in range(len(Q)):\n",
        "            matrix[i,j] = (Q[j] - C[i])**2\n",
        "    distances = np.asmatrix(matrix)\n",
        "    \n",
        "    return distances\n",
        "\n",
        "\n",
        "## Plot the Distance Cost Plot\n",
        "def distance_cost_plot(distances):\n",
        "    im = plt.imshow(distances, interpolation='nearest', cmap='Reds') \n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.xlabel(\"X\")\n",
        "    plt.ylabel(\"Y\")\n",
        "    plt.grid()\n",
        "    plt.colorbar();\n",
        "\n",
        "def accumulated_costs(Q,C, distances):\n",
        "    accumulated_cost = np.zeros((len(C), len(Q)))\n",
        "    accumulated_cost[0,0] = distances[0,0]\n",
        "    \n",
        "    # First Row Only\n",
        "    for i in range(1, len(Q)):\n",
        "        accumulated_cost[0,i] = distances[0,i] + accumulated_cost[0, i-1]\n",
        "    # First Column Only\n",
        "    for i in range(1, len(C)):\n",
        "        accumulated_cost[i,0] = distances[i, 0] + accumulated_cost[i-1, 0]\n",
        "    # All other Elements\n",
        "    for i in range(1, len(C)):\n",
        "        for j in range(1, len(Q)):\n",
        "            accumulated_cost[i, j] = min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]) + distances[i, j]\n",
        "    \n",
        "    return accumulated_cost\n",
        "\n",
        "def backtrack(Q, C, accumulated_cost, plotting=True):\n",
        "    path = [[len(Q)-1, len(C)-1]]\n",
        "    i = len(C)-1\n",
        "    j = len(Q)-1\n",
        "    while i>0 and j>0:\n",
        "        if i==0:\n",
        "            j = j - 1\n",
        "        elif j==0:\n",
        "            i = i - 1\n",
        "        else:\n",
        "            if accumulated_cost[i-1, j] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n",
        "                i = i - 1\n",
        "            elif accumulated_cost[i, j-1] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n",
        "                j = j-1\n",
        "            else:\n",
        "                i = i - 1\n",
        "                j= j- 1\n",
        "        path.append([j, i])\n",
        "    path.append([0,0])\n",
        "\n",
        "    path_x = [point[0] for point in path]\n",
        "    path_y = [point[1] for point in path]\n",
        "\n",
        "  \n",
        "    if plotting == True:\n",
        "        distance_cost_plot(accumulated_cost)\n",
        "        plt.plot(path_x, path_y)\n",
        "\n",
        "    return path\n",
        "\n",
        "def path_cost(Q, C, accumulated_cost, distances):\n",
        "    path = [[len(Q)-1, len(C)-1]]\n",
        "    cost = 0\n",
        "    i = len(C)-1\n",
        "    j = len(Q)-1\n",
        "    while i>0 and j>0:\n",
        "        if i==0:\n",
        "            j = j - 1\n",
        "        elif j==0:\n",
        "            i = i - 1\n",
        "        else:\n",
        "            if accumulated_cost[i-1, j] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n",
        "                i = i - 1\n",
        "            elif accumulated_cost[i, j-1] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n",
        "                j = j-1\n",
        "            else:\n",
        "                i = i - 1\n",
        "                j= j- 1\n",
        "        path.append([j, i])\n",
        "    path.append([0,0])\n",
        "\n",
        "    for [C, Q] in path:\n",
        "        cost = cost +distances[Q, C]\n",
        "\n",
        "    return(path, cost)\n",
        "\n",
        "def distance_DTWd(Q, C):\n",
        "    matrix = np.ones((len(C[0]), len(Q[0])))\n",
        "    for i in range(len(C[0])):\n",
        "        for j in range(len(Q[0])):\n",
        "            d = 0\n",
        "            for M in range(len(Q)):\n",
        "                d += ((Q[M][j] - C[M][i])**2)\n",
        "            matrix[i,j] = d\n",
        "  \n",
        "    distances = np.asmatrix(matrix)\n",
        "    return distances\n",
        "\n",
        "def DTW_i(Q, C):\n",
        "    c = 0\n",
        "    p = []\n",
        "    for i in range(len(Q)):\n",
        "        distance =  distance_matrix(Q[i], C[i])\n",
        "        acc_costs = accumulated_costs(Q[i],C[i],distance)\n",
        "        path = backtrack(Q[i],C[i], acc_costs, plotting=False)\n",
        "        paths, cost = path_cost(Q[i], C[i], acc_costs, distance)\n",
        "        c += cost\n",
        "    return(c)\n",
        "\n",
        "def DTW_d(Q, C):\n",
        "    c = []\n",
        "    p = []\n",
        "    for i in range(len(Q)):\n",
        "        distance = distance_DTWd(Q,C)\n",
        "        acc_costs = accumulated_costs(Q[i],C[i],distance)\n",
        "        path = backtrack(Q[i],C[i], acc_costs, plotting=False)\n",
        "        paths, cost = path_cost(Q[i], C[i], acc_costs, distance)\n",
        "        c.append(cost)\n",
        "\n",
        "    return(np.min(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4bNAzKv-gmg"
      },
      "source": [
        "def evaluate_dtw_sample(gen, real):\n",
        "    d=[]\n",
        "    real = real[:,:,:,0]\n",
        "    gen = gen[:,:,:,0]\n",
        "    j=0\n",
        "    sample = real[j].permute(1,0)\n",
        "    gen_data = gen[j].permute(1,0)\n",
        "    # Compute DTW_d\n",
        "    d.append(DTW_d(gen_data.detach().cpu().numpy(), sample.detach().cpu().numpy()))\n",
        "    # Option for DTW_i\n",
        "    #d.append(DTW_i(gen_data.detach().cpu().numpy(), sample.detach().cpu().numpy()))\n",
        "    D = np.mean(d)\n",
        "    \n",
        "    return D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZql4aWR9pqC"
      },
      "source": [
        "### SOFTDTW\n",
        "\n",
        "Can choose to use this function in DTWGAN if you want"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wdpObibVrJ-"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from numba import jit\n",
        "from torch.autograd import Function\n",
        "\n",
        "@jit(nopython = True)\n",
        "def compute_softdtw(D, gamma):\n",
        "  B = D.shape[0]\n",
        "  N = D.shape[1]\n",
        "  M = D.shape[2]\n",
        "  R = np.ones((B, N + 2, M + 2)) * np.inf\n",
        "  R[:, 0, 0] = 0\n",
        "  for k in range(B):\n",
        "    for j in range(1, M + 1):\n",
        "      for i in range(1, N + 1):\n",
        "        r0 = -R[k, i - 1, j - 1] / gamma\n",
        "        r1 = -R[k, i - 1, j] / gamma\n",
        "        r2 = -R[k, i, j - 1] / gamma\n",
        "        rmax = max(max(r0, r1), r2)\n",
        "        rsum = np.exp(r0 - rmax) + np.exp(r1 - rmax) + np.exp(r2 - rmax)\n",
        "        softmin = - gamma * (np.log(rsum) + rmax)\n",
        "        R[k, i, j] = D[k, i - 1, j - 1] + softmin\n",
        "  return R\n",
        "\n",
        "@jit(nopython = True)\n",
        "def compute_softdtw_backward(D_, R, gamma):\n",
        "  B = D_.shape[0]\n",
        "  N = D_.shape[1]\n",
        "  M = D_.shape[2]\n",
        "  D = np.zeros((B, N + 2, M + 2))\n",
        "  E = np.zeros((B, N + 2, M + 2))\n",
        "  D[:, 1:N + 1, 1:M + 1] = D_\n",
        "  E[:, -1, -1] = 1\n",
        "  R[:, : , -1] = -np.inf\n",
        "  R[:, -1, :] = -np.inf\n",
        "  R[:, -1, -1] = R[:, -2, -2]\n",
        "  for k in range(B):\n",
        "    for j in range(M, 0, -1):\n",
        "      for i in range(N, 0, -1):\n",
        "        a0 = (R[k, i + 1, j] - R[k, i, j] - D[k, i + 1, j]) / gamma\n",
        "        b0 = (R[k, i, j + 1] - R[k, i, j] - D[k, i, j + 1]) / gamma\n",
        "        c0 = (R[k, i + 1, j + 1] - R[k, i, j] - D[k, i + 1, j + 1]) / gamma\n",
        "        a = np.exp(a0)\n",
        "        b = np.exp(b0)\n",
        "        c = np.exp(c0)\n",
        "        E[k, i, j] = E[k, i + 1, j] * a + E[k, i, j + 1] * b + E[k, i + 1, j + 1] * c\n",
        "  return E[:, 1:N + 1, 1:M + 1]\n",
        "\n",
        "class _SoftDTW(Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, D, gamma):\n",
        "    dev = D.device\n",
        "    dtype = D.dtype\n",
        "    gamma = torch.Tensor([gamma]).to(dev).type(dtype) # dtype fixed\n",
        "    D_ = D.detach().cpu().numpy()\n",
        "    g_ = gamma.item()\n",
        "    R = torch.Tensor(compute_softdtw(D_, g_)).to(dev).type(dtype)\n",
        "    ctx.save_for_backward(D, R, gamma)\n",
        "    return R[:, -2, -2]\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    dev = grad_output.device\n",
        "    dtype = grad_output.dtype\n",
        "    D, R, gamma = ctx.saved_tensors\n",
        "    D_ = D.detach().cpu().numpy()\n",
        "    R_ = R.detach().cpu().numpy()\n",
        "    g_ = gamma.item()\n",
        "    E = torch.Tensor(compute_softdtw_backward(D_, R_, g_)).to(dev).type(dtype)\n",
        "    return grad_output.view(-1, 1, 1).expand_as(E) * E, None\n",
        "\n",
        "class SoftDTW(torch.nn.Module):\n",
        "  def __init__(self, gamma=1.0, normalize=False):\n",
        "    super(SoftDTW, self).__init__()\n",
        "    self.normalize = normalize\n",
        "    self.gamma=gamma\n",
        "    self.func_dtw = _SoftDTW.apply\n",
        "\n",
        "  def calc_distance_matrix(self, x, y):\n",
        "    n = x.size(1)\n",
        "    m = y.size(1)\n",
        "    d = x.size(2)\n",
        "    x = x.unsqueeze(2).expand(-1, n, m, d)\n",
        "    y = y.unsqueeze(1).expand(-1, n, m, d)\n",
        "    dist = torch.pow(x - y, 2).sum(3)\n",
        "    return dist\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    assert len(x.shape) == len(y.shape)\n",
        "    squeeze = False\n",
        "    if len(x.shape) < 3:\n",
        "      x = x.unsqueeze(0)\n",
        "      y = y.unsqueeze(0)\n",
        "      squeeze = True\n",
        "    if self.normalize:\n",
        "      D_xy = self.calc_distance_matrix(x, y)\n",
        "      out_xy = self.func_dtw(D_xy, self.gamma)\n",
        "      D_xx = self.calc_distance_matrix(x, x)\n",
        "      out_xx = self.func_dtw(D_xx, self.gamma)\n",
        "      D_yy = self.calc_distance_matrix(y, y)\n",
        "      out_yy = self.func_dtw(D_yy, self.gamma)\n",
        "      result = out_xy - 1/2 * (out_xx + out_yy) # distance\n",
        "    else:\n",
        "      D_xy = self.calc_distance_matrix(x, y)\n",
        "      out_xy = self.func_dtw(D_xy, self.gamma)\n",
        "      result = out_xy # discrepancy\n",
        "    return result.squeeze(0) if squeeze else result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQrQ_xQcPqCF"
      },
      "source": [
        "## GAN Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocke51Wh_AuT"
      },
      "source": [
        "### LSGAN\n",
        "\n",
        "Standard Least Squares GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SehreEKvwe5i"
      },
      "source": [
        "# !!! Minimizes MSE instead of BCE\n",
        "adversarial_loss = torch.nn.MSELoss()\n",
        "\n",
        "def train_LSgan(filename, params, savepath):\n",
        "    # Load data\n",
        "    data_loader, num_batches = load_data(filename, params['batch_size'])\n",
        "    # Save Parameters\n",
        "    save_params(params, savepath)\n",
        "    \n",
        "    # Iterative through the MBD Layers\n",
        "    for mbd in params['minibatch_layer']:\n",
        "        print(\"MBD_Layer: \"+str(mbd))\n",
        "        G_losses = []\n",
        "        D_losses = []\n",
        "        \n",
        "        # Load model for this MBD layer\n",
        "        generator, discriminator, g_optimizer, d_optimizer = load_model(params, int(mbd))\n",
        "        for n in tqdm(range(params['epochs'])):\n",
        "            \n",
        "            for n_batch, sample_data in enumerate(data_loader):\n",
        "                if len(sample_data[:,0,0]) < params['batch_size']:\n",
        "                    break\n",
        "                else:\n",
        "                    # Adversarial GT\n",
        "                    valid = Variable(Tensor(sample_data.size(0), 2).fill_(1.0), requires_grad=False)\n",
        "                    fake = Variable(Tensor(sample_data.size(0), 2).fill_(0.0), requires_grad=False)\n",
        "\n",
        "                    # -----------------\n",
        "                    #  Train Generator\n",
        "                    # -----------------\n",
        "                    g_optimizer.zero_grad()\n",
        "                    h_g = generator.init_hidden()\n",
        "                    \n",
        "                    # Sample noise as generator input\n",
        "                    noise_sample = Variable(noise(len(sample_data), params['seq_len'])).to(device)\n",
        "\n",
        "                    # Generate a batch of fake data\n",
        "                    fake_data = generator.forward(noise_sample,h_g)#.detach()\n",
        "                    fake_data = fake_data.view(params['batch_size'], -1, params['seq_len'], 1)\n",
        "\n",
        "                    # Loss measures generator's ability to fool the discriminator\n",
        "                    g_loss = adversarial_loss(discriminator(fake_data), valid)\n",
        "\n",
        "                    g_loss.backward()\n",
        "                    g_optimizer.step()  \n",
        "                    \n",
        "                    # ---------------------\n",
        "                    #  Train Discriminator\n",
        "                    # ---------------------\n",
        "                    d_optimizer.zero_grad()\n",
        "\n",
        "                    # Generate a batch of real data   \n",
        "                    real_data = Variable(sample_data.float()).to(device)  \n",
        "                    real_data = real_data.view(params['batch_size'], -1, params['seq_len'], 1)\n",
        "\n",
        "                    # Measure discriminator's ability to classify real from generated samples\n",
        "                    real_loss = adversarial_loss(discriminator(real_data), valid)\n",
        "                    fake_loss = adversarial_loss(discriminator(fake_data.detach()), fake)\n",
        "\n",
        "                    d_loss = (0.5 * (real_loss + fake_loss))\n",
        "\n",
        "                    d_loss.backward()\n",
        "                    d_optimizer.step()\n",
        "\n",
        "\n",
        "                if n_batch == (num_batches - 2):\n",
        "                    G_losses.append(g_loss.item())\n",
        "                    D_losses.append(d_loss.item())\n",
        "                  \n",
        "                    torch.save(generator.state_dict(), savepath+'/MBD_' +str(mbd)+ '/gen/generator_state_'+str(n)+'.pt')\n",
        "                    torch.save(discriminator.state_dict(), savepath+'/MBD_' +str(mbd)+ '/disc/discriminator_state_'+str(n)+'.pt')\n",
        "                                   \n",
        "        # Dumping the errors for each training epoch.\n",
        "        with open(savepath+'/MBD_'+str(mbd)+'/generator_losses.txt', 'wb') as fp:\n",
        "            pickle.dump(G_losses, fp)\n",
        "        with open(savepath+'/MBD_' +str(mbd)+ '/discriminator_losses.txt', 'wb') as fp:\n",
        "            pickle.dump(D_losses, fp) \n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqkMV5CVPKtf"
      },
      "source": [
        "### LSGAN-DTW \n",
        "Implemented with R multivariate DTW package. You can replace `R_DTW(G(z), x)` function with `evaluate_dtw_sample(G(z), x)` for our adapted DTW method but it is much slower."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9qds6K8POqE"
      },
      "source": [
        "# !!! Minimizes MSE instead of BCE\n",
        "adversarial_loss = torch.nn.MSELoss()\n",
        "\n",
        "def train_RDTWGAN(filename, params, savepath):\n",
        "    # Load data\n",
        "    data_loader, num_batches = load_data(filename, params['batch_size'])\n",
        "    # Save Parameters\n",
        "    save_params(params, savepath)\n",
        "    \n",
        "    # Iterative through the MBD Layers\n",
        "    for mbd in params['minibatch_layer']:\n",
        "        print(\"MBD_Layer: \"+str(mbd))\n",
        "        print(\"DL_Length = \" +str(len((data_loader))))\n",
        "        G_losses = []\n",
        "        D_losses = []\n",
        "        \n",
        "        # Load model for this MBD layer\n",
        "        generator, discriminator, g_optimizer, d_optimizer = load_model(params, int(mbd))\n",
        "        for n in tqdm(range(params['epochs'])):\n",
        "            for n_batch, sample_data in enumerate(data_loader):\n",
        "                if len(sample_data[:,0,0]) < params['batch_size']:\n",
        "                    break\n",
        "                else:\n",
        "                    # Adversarial GT\n",
        "                    valid = Variable(Tensor(sample_data.size(0), 2).fill_(1.0), requires_grad=False)\n",
        "                    fake = Variable(Tensor(sample_data.size(0), 2).fill_(0.0), requires_grad=False)\n",
        "\n",
        "                    # -----------------\n",
        "                    #  Train Generator\n",
        "                    # -----------------\n",
        "                    g_optimizer.zero_grad()\n",
        "                    h_g = generator.init_hidden()\n",
        "                    \n",
        "                    # Sample noise as generator input\n",
        "                    noise_sample = Variable(noise(len(sample_data), params['seq_len'])).to(device)\n",
        "\n",
        "                    # Generate a batch of fake data\n",
        "                    fake_data = generator.forward(noise_sample,h_g)#.detach()\n",
        "                    fake_data = fake_data.view(params['batch_size'], -1, params['seq_len'], 1)\n",
        "\n",
        "                    # Loss measures generator's ability to fool the discriminator\n",
        "                    g_loss = adversarial_loss(discriminator(fake_data), valid)\n",
        "\n",
        "                    g_loss.backward()\n",
        "                    g_optimizer.step()  \n",
        "                    \n",
        "                    # ---------------------\n",
        "                    #  Train Discriminator\n",
        "                    # ---------------------\n",
        "                    d_optimizer.zero_grad()\n",
        "\n",
        "                    # Generate a batch of real data   \n",
        "                    real_data = Variable(sample_data.float()).to(device)  \n",
        "                    real_data = real_data.view(params['batch_size'], -1, params['seq_len'], 1)\n",
        "\n",
        "                    # Measure discriminator's ability to classify real from generated samples\n",
        "                    real_loss = adversarial_loss(discriminator(real_data), valid)\n",
        "                    fake_loss = adversarial_loss(discriminator(fake_data.detach()), fake)\n",
        "                    dtw = R_DTW(fake_data, real_data)\n",
        "                    dtw_loss = 1 - (1.0/math.log(dtw))\n",
        "\n",
        "                    #dtw_loss = torch.tensor(dtw_loss, dtype=torch.float64)\n",
        "\n",
        "                    d_loss = (0.5 * (real_loss + fake_loss) + dtw_loss)\n",
        " \n",
        "                    d_loss.backward()\n",
        "                    d_optimizer.step()\n",
        "\n",
        "                    if n_batch % 50 == 0:\n",
        "                      print(\"Batch: \" +str(n_batch)+ \"/\"+str(len(data_loader)))\n",
        "                      print(\"D_loss: \"+str(d_loss))\n",
        "\n",
        "\n",
        "                if n_batch == (num_batches - 2):\n",
        "                    G_losses.append(g_loss.item())\n",
        "                    D_losses.append(d_loss.item())\n",
        "                  \n",
        "                    torch.save(generator.state_dict(), savepath+'/MBD_' +str(mbd)+ '/gen/generator_state_'+str(n)+'.pt')\n",
        "                    torch.save(discriminator.state_dict(), savepath+'/MBD_' +str(mbd)+ '/disc/discriminator_state_'+str(n)+'.pt')\n",
        "                                   \n",
        "        # Dumping the errors for each training epoch.\n",
        "        with open(savepath+'/MBD_'+str(mbd)+'/generator_losses.txt', 'wb') as fp:\n",
        "            pickle.dump(G_losses, fp)\n",
        "        with open(savepath+'/MBD_' +str(mbd)+ '/discriminator_losses.txt', 'wb') as fp:\n",
        "            pickle.dump(D_losses, fp) \n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tB00unuxqzM"
      },
      "source": [
        "###DTWGAN\n",
        "\n",
        "Can use whatever DTW package here. For the paper we use our `evaluate_dtw_sample`, here `SoftDTW` is implmented as it is differentiable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0igPaJ21Bsy"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "import math\n",
        "import chainer.functions as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgJyOlghxvVg"
      },
      "source": [
        "# !!! Minimizes DTW distance\n",
        "criterion = SoftDTW(gamma=1.0, normalize=True) # just like nn.MSELoss()\n",
        "\n",
        "\n",
        "def train_LSoftDTWgan(filename, params, savepath):\n",
        "    # Load data\n",
        "    data_loader, num_batches = load_data(filename, params['batch_size'])\n",
        "    # Save Parameters\n",
        "    save_params(params, savepath)\n",
        "    \n",
        "    # Iterative through the MBD Layers\n",
        "    for mbd in params['minibatch_layer']:\n",
        "        print(\"MBD_Layer: \"+str(mbd))\n",
        "        G_losses = []\n",
        "        D_losses = []\n",
        "        \n",
        "        # Load model for this MBD layer\n",
        "        generator, discriminator, g_optimizer, d_optimizer = load_model(params, int(mbd))\n",
        "        for n in tqdm(range(params['epochs'])):\n",
        "            for n_batch, sample_data in enumerate(data_loader):\n",
        "                if len(sample_data[:,0,0]) < params['batch_size']:\n",
        "                    break\n",
        "                else:\n",
        "                    # Adversarial GT\n",
        "                    valid = Variable(Tensor(sample_data.size(0), 2).fill_(1.0), requires_grad=False)\n",
        "                    fake = Variable(Tensor(sample_data.size(0), 2).fill_(0.0), requires_grad=False)\n",
        "                    \n",
        "                    # -----------------\n",
        "                    #  Train Generator\n",
        "                    # -----------------\n",
        "                    g_optimizer.zero_grad()\n",
        "                    h_g = generator.init_hidden()\n",
        "                    \n",
        "                    # Sample noise as generator input\n",
        "                    noise_sample = Variable(noise(len(sample_data), params['seq_len'])).to(device)\n",
        "\n",
        "                    # Generate a batch of fake data\n",
        "                    fake_data = generator.forward(noise_sample,h_g)#.detach()\n",
        "                    fake_data = fake_data.view(params['batch_size'], -1, params['seq_len'], 1)\n",
        "\n",
        "                    g_loss = 0.5*criterion(discriminator(fake_data), valid)\n",
        "                    g_loss.backward()\n",
        "                    g_optimizer.step()  \n",
        "                    \n",
        "                    # ---------------------\n",
        "                    #  Train Discriminator\n",
        "                    # ---------------------\n",
        "                    d_optimizer.zero_grad()\n",
        "\n",
        "                    # Generate a batch of real data   \n",
        "                    real_data = Variable(sample_data.float()).to(device)  \n",
        "                    real_data = real_data.view(params['batch_size'], -1, params['seq_len'], 1)\n",
        "  \n",
        "                    real_loss = criterion(discriminator(real_data), valid)\n",
        "                    fake_loss = criterion(discriminator(fake_data.detach()), fake)\n",
        "\n",
        "\n",
        "                    if n_batch % 50 == 0:\n",
        "                      print(\"Batch: \" +str(n_batch)+ \"/\"+str(len(data_loader)))\n",
        "                    \n",
        "                    d_loss = (0.5 * (real_loss + fake_loss))\n",
        "                    #d_loss = torch.tensor(d_loss, requires_grad=True).cuda()\n",
        "\n",
        "                    d_loss.backward()\n",
        "                    d_optimizer.step()\n",
        "\n",
        "\n",
        "                if n_batch == (num_batches - 2):\n",
        "                    print(g_loss, d_loss)\n",
        "                    G_losses.append(g_loss.item())\n",
        "                    D_losses.append(d_loss.item())\n",
        "                  \n",
        "                    torch.save(generator.state_dict(), savepath+'/MBD_' +str(mbd)+ '/gen/generator_state_'+str(n)+'.pt')\n",
        "                    torch.save(discriminator.state_dict(), savepath+'/MBD_' +str(mbd)+ '/disc/discriminator_state_'+str(n)+'.pt')\n",
        "                                   \n",
        "        # Dumping the errors for each training epoch.\n",
        "        with open(savepath+'/MBD_'+str(mbd)+'/generator_losses.txt', 'wb') as fp:\n",
        "            pickle.dump(G_losses, fp)\n",
        "        with open(savepath+'/MBD_' +str(mbd)+ '/discriminator_losses.txt', 'wb') as fp:\n",
        "            pickle.dump(D_losses, fp) \n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCQjK7_9_Gsm"
      },
      "source": [
        "### LS-DTWGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj-wUQ38_Dby"
      },
      "source": [
        "# !!! Minimizes MSE instead of BCE\n",
        "adversarial_loss = torch.nn.MSELoss()\n",
        "\n",
        "def train_LSDTWgan(filename, params, savepath):\n",
        "    # Load data\n",
        "    data_loader, num_batches = load_data(filename, params['batch_size'])\n",
        "    # Save Parameters\n",
        "    save_params(params, savepath)\n",
        "    \n",
        "    # Iterative through the MBD Layers\n",
        "    for mbd in params['minibatch_layer']:\n",
        "        print(\"MBD_Layer: \"+str(mbd))\n",
        "        print(\"DL_Length = \" +str(len((data_loader))))\n",
        "        G_losses = []\n",
        "        D_losses = []\n",
        "        \n",
        "        # Load model for this MBD layer\n",
        "        generator, discriminator, g_optimizer, d_optimizer = load_model(params, int(mbd))\n",
        "        for n in tqdm(range(params['epochs'])):\n",
        "            for n_batch, sample_data in enumerate(data_loader):\n",
        "                if len(sample_data[:,0,0]) < params['batch_size']:\n",
        "                    break\n",
        "                else:\n",
        "                    # Adversarial GT\n",
        "                    valid = Variable(Tensor(sample_data.size(0), 2).fill_(1.0), requires_grad=False)\n",
        "                    fake = Variable(Tensor(sample_data.size(0), 2).fill_(0.0), requires_grad=False)\n",
        "\n",
        "                    # -----------------\n",
        "                    #  Train Generator\n",
        "                    # -----------------\n",
        "                    g_optimizer.zero_grad()\n",
        "                    h_g = generator.init_hidden()\n",
        "                    \n",
        "                    # Sample noise as generator input\n",
        "                    noise_sample = Variable(noise(len(sample_data), params['seq_len'])).to(device)\n",
        "\n",
        "                    # Generate a batch of fake data\n",
        "                    fake_data = generator.forward(noise_sample,h_g)#.detach()\n",
        "                    fake_data = fake_data.view(params['batch_size'], -1, params['seq_len'], 1)\n",
        "\n",
        "                    # Loss measures generator's ability to fool the discriminator\n",
        "                    g_loss = adversarial_loss(discriminator(fake_data), valid)\n",
        "\n",
        "                    g_loss.backward()\n",
        "                    g_optimizer.step()  \n",
        "                    \n",
        "                    # ---------------------\n",
        "                    #  Train Discriminator\n",
        "                    # ---------------------\n",
        "                    d_optimizer.zero_grad()\n",
        "\n",
        "                    # Generate a batch of real data   \n",
        "                    real_data = Variable(sample_data.float()).to(device)  \n",
        "                    real_data = real_data.view(params['batch_size'], -1, params['seq_len'], 1)\n",
        "\n",
        "                    # Measure discriminator's ability to classify real from generated samples\n",
        "                    real_loss = adversarial_loss(discriminator(real_data), valid)\n",
        "                    fake_loss = adversarial_loss(discriminator(fake_data.detach()), fake)\n",
        "                    dtw_loss = evaluate_dtw_sample(fake_data, real_data)\n",
        "                    d_loss = (0.5 * (real_loss + fake_loss)) + (0.0001 * dtw_loss)\n",
        "                    #d_loss = (0.3 * (real_loss + fake_loss + math.log(dtw_loss)))\n",
        " \n",
        "                    d_loss.backward()\n",
        "                    d_optimizer.step()\n",
        "\n",
        "                    if n_batch % 50 == 0:\n",
        "                      print(\"Batch: \" +str(n_batch)+ \"/\"+str(len(data_loader)))\n",
        "\n",
        "\n",
        "                if n_batch == (num_batches - 2):\n",
        "                    G_losses.append(g_loss.item())\n",
        "                    D_losses.append(d_loss.item())\n",
        "                  \n",
        "                    torch.save(generator.state_dict(), savepath+'/MBD_' +str(mbd)+ '/gen/generator_state_'+str(n)+'.pt')\n",
        "                    torch.save(discriminator.state_dict(), savepath+'/MBD_' +str(mbd)+ '/disc/discriminator_state_'+str(n)+'.pt')\n",
        "                                   \n",
        "        # Dumping the errors for each training epoch.\n",
        "        with open(savepath+'/MBD_'+str(mbd)+'/generator_losses.txt', 'wb') as fp:\n",
        "            pickle.dump(G_losses, fp)\n",
        "        with open(savepath+'/MBD_' +str(mbd)+ '/discriminator_losses.txt', 'wb') as fp:\n",
        "            pickle.dump(D_losses, fp) \n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayJkFQOud5Ef"
      },
      "source": [
        "### LS-GAN\n",
        "\n",
        "Loss Sense GAN with L2 distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o3osqVTd4Gf"
      },
      "source": [
        "# -------- Init tensor --------\n",
        "adversarial_loss = torch.nn.MSELoss()\n",
        "\n",
        "slope=0.0\n",
        "LeakyReLU = torch.nn.LeakyReLU(slope).to(device)\n",
        "lamb = 2e-4\n",
        "l2dist = torch.nn.PairwiseDistance(2)\n",
        "\n",
        "\n",
        "def get_direct_gradient_penalty(netD, x, gamma, cuda):\n",
        "    if cuda:\n",
        "        x = x.cuda()\n",
        "\n",
        "    x = autograd.Variable(x, requires_grad=True)\n",
        "    output = netD(x)\n",
        "    gradOutput = torch.ones(output.size()).cuda() if cuda else torch.ones(output.size())\n",
        "    \n",
        "    gradient = torch.autograd.grad(outputs=output, inputs=x, grad_outputs=gradOutput, create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "    gradientPenalty = (gradient.norm(2, dim=1)).mean() * gamma\n",
        "    \n",
        "    return gradientPenalty\n",
        "\n",
        "def LS_GAN(filename, params, savepath):\n",
        "    # Load data\n",
        "    data_loader, num_batches = load_data(filename, params['batch_size'])\n",
        "    # Save Parameters\n",
        "    save_params(params, savepath)\n",
        "    \n",
        "    # Iterative through the MBD Layers\n",
        "    for mbd in params['minibatch_layer']:\n",
        "        print(\"MBD_Layer: \"+str(mbd))\n",
        "        G_losses = []\n",
        "        D_losses = []\n",
        "        \n",
        "        # Load model for this MBD layer\n",
        "        generator, discriminator, g_optimizer, d_optimizer = load_model(params, int(mbd))\n",
        "        for n in tqdm(range(params['epochs'])):\n",
        "            \n",
        "            for n_batch, sample_data in enumerate(data_loader):\n",
        "                if len(sample_data[:,0,0]) < params['batch_size']:\n",
        "                    break\n",
        "                else:\n",
        "                    # Adversarial GT\n",
        "                    valid = Variable(Tensor(sample_data.size(0), 2).fill_(1.0), requires_grad=False)\n",
        "                    fake = Variable(Tensor(sample_data.size(0), 2).fill_(0.0), requires_grad=False)\n",
        "                    # ---------------------\n",
        "                    #  Train Discriminator\n",
        "                    # ---------------------\n",
        "                    # Update D network\n",
        "                    for p in discriminator.parameters():\n",
        "                        p.requires_grad = True \n",
        "\n",
        "                    d_optimizer.zero_grad()\n",
        "                    h_g = generator.init_hidden()\n",
        "                    # Generate a batch of real data   \n",
        "                    real_data = Variable(sample_data.float()).to(device)  \n",
        "                    real_data = real_data.view(params['batch_size'], -1, params['seq_len'], 1)\n",
        "\n",
        "                    # Sample noise as generator input\n",
        "                    noise_sample = Variable(noise(len(sample_data), params['seq_len'])).to(device)\n",
        "\n",
        "                    # Generate a batch of fake data\n",
        "                    fake_data = generator.forward(noise_sample,h_g)#.detach()\n",
        "                    fake_data = fake_data.view(params['batch_size'], -1, params['seq_len'], 1)\n",
        "\n",
        "                    # Loss R for real\n",
        "                    LossR = discriminator(real_data)\n",
        "                    # Loss F for fake.\n",
        "                    LossF = discriminator(fake_data)\n",
        "            \n",
        "                    pdist = l2dist(real_data[:,:,:,0].view(params['batch_size'], -1, 2), fake_data[:,:,:,0].view(params['batch_size'], -1, 2)).mul(lamb).to(device)\n",
        " \n",
        "                    # Loss for D.\n",
        "                    d_loss = LeakyReLU(LossR - LossF + pdist).mean()\n",
        "                    d_loss.backward()\n",
        "\n",
        "                    #gp = get_direct_gradient_penalty(discriminator, real_data, 10, True)\n",
        "                    #gp.backward()\n",
        "\n",
        "                    # Gradient of D.\n",
        "                    gradD = real_data.grad\n",
        "\n",
        "                    d_optimizer.step() \n",
        "\n",
        "                    # -----------------\n",
        "                    #  Train Generator\n",
        "                    # -----------------\n",
        "                    # Update G network, freeze D.\n",
        "                    for p in discriminator.parameters():\n",
        "                        p.requires_grad = False \n",
        "\n",
        "                    g_optimizer.zero_grad()\n",
        "                    h_g = generator.init_hidden()\n",
        "\n",
        "                    # Sample noise as generator input\n",
        "                    noise_sample = Variable(noise(len(sample_data), params['seq_len'])).to(device)\n",
        "\n",
        "                    # Generate a batch of fake data\n",
        "                    fake_data = generator.forward(noise_sample,h_g)\n",
        "                    fake_data = fake_data.view(params['batch_size'], -1, params['seq_len'], 1)\n",
        "\n",
        "                    # Loss F for fake.\n",
        "                    LossF = discriminator(fake_data)\n",
        "                    # Loss measures generator's ability to fool the discriminator\n",
        "                    g_loss = LossF.mean()\n",
        "\n",
        "                    g_loss.backward()\n",
        "                    g_optimizer.step() \n",
        "\n",
        "\n",
        "                if n_batch == (num_batches - 2):\n",
        "                    G_losses.append(g_loss.item())\n",
        "                    D_losses.append(d_loss.item())\n",
        "                  \n",
        "                    torch.save(generator.state_dict(), savepath+'/MBD_' +str(mbd)+ '/gen/generator_state_'+str(n)+'.pt')\n",
        "                    torch.save(discriminator.state_dict(), savepath+'/MBD_' +str(mbd)+ '/disc/discriminator_state_'+str(n)+'.pt')\n",
        "                                   \n",
        "        # Dumping the errors for each training epoch.\n",
        "        with open(savepath+'/MBD_'+str(mbd)+'/generator_losses.txt', 'wb') as fp:\n",
        "            pickle.dump(G_losses, fp)\n",
        "        with open(savepath+'/MBD_' +str(mbd)+ '/discriminator_losses.txt', 'wb') as fp:\n",
        "            pickle.dump(D_losses, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWG8tb9QOTWs"
      },
      "source": [
        "### LS-GAN-DTW\n",
        "\n",
        "Loss-Sense GAN with DTW distance "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnEFMcPpObCQ"
      },
      "source": [
        "# -------- Init tensor --------\n",
        "adversarial_loss = torch.nn.MSELoss()\n",
        "\n",
        "slope=0.0\n",
        "LeakyReLU = torch.nn.LeakyReLU(slope).to(device)\n",
        "lamb = 2e-4\n",
        "l2dist = torch.nn.PairwiseDistance(2)\n",
        "\n",
        "\n",
        "def get_direct_gradient_penalty(netD, x, gamma, cuda):\n",
        "    if cuda:\n",
        "        x = x.cuda()\n",
        "\n",
        "    x = autograd.Variable(x, requires_grad=True)\n",
        "    output = netD(x)\n",
        "    gradOutput = torch.ones(output.size()).cuda() if cuda else torch.ones(output.size())\n",
        "    \n",
        "    gradient = torch.autograd.grad(outputs=output, inputs=x, grad_outputs=gradOutput, create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "    gradientPenalty = (gradient.norm(2, dim=1)).mean() * gamma\n",
        "    \n",
        "    return gradientPenalty\n",
        "\n",
        "def LS_GAN(filename, params, savepath):\n",
        "    # Load data\n",
        "    data_loader, num_batches = load_data(filename, params['batch_size'])\n",
        "    # Save Parameters\n",
        "    save_params(params, savepath)\n",
        "    \n",
        "    # Iterative through the MBD Layers\n",
        "    for mbd in params['minibatch_layer']:\n",
        "        print(\"MBD_Layer: \"+str(mbd))\n",
        "        G_losses = []\n",
        "        D_losses = []\n",
        "        \n",
        "        # Load model for this MBD layer\n",
        "        generator, discriminator, g_optimizer, d_optimizer = load_model(params, int(mbd))\n",
        "        for n in tqdm(range(params['epochs'])):\n",
        "            \n",
        "            for n_batch, sample_data in enumerate(data_loader):\n",
        "                if len(sample_data[:,0,0]) < params['batch_size']:\n",
        "                    break\n",
        "                else:\n",
        "                    # Adversarial GT\n",
        "                    valid = Variable(Tensor(sample_data.size(0), 2).fill_(1.0), requires_grad=False)\n",
        "                    fake = Variable(Tensor(sample_data.size(0), 2).fill_(0.0), requires_grad=False)\n",
        "                    # ---------------------\n",
        "                    #  Train Discriminator\n",
        "                    # ---------------------\n",
        "                    # Update D network\n",
        "                    for p in discriminator.parameters():\n",
        "                        p.requires_grad = True \n",
        "\n",
        "                    d_optimizer.zero_grad()\n",
        "                    h_g = generator.init_hidden()\n",
        "                    # Generate a batch of real data   \n",
        "                    real_data = Variable(sample_data.float()).to(device)  \n",
        "                    real_data = real_data.view(params['batch_size'], -1, params['seq_len'], 1)\n",
        "\n",
        "                    # Sample noise as generator input\n",
        "                    noise_sample = Variable(noise(len(sample_data), params['seq_len'])).to(device)\n",
        "\n",
        "                    # Generate a batch of fake data\n",
        "                    fake_data = generator.forward(noise_sample,h_g)\n",
        "                    fake_data = fake_data.view(params['batch_size'], -1, params['seq_len'], 1)\n",
        "\n",
        "                    # Loss R for real\n",
        "                    LossR = discriminator(real_data)\n",
        "                    # Loss F for fake.\n",
        "                    LossF = discriminator(fake_data)\n",
        "\n",
        "                    #x = torch.FloatTensor(R_DTW(real_data, fake_data)).mul(lamb).to(device) #R_DTW(fake_data, real_data)\n",
        "                    #pdist = torch.zeros(50, 2).to(device)\n",
        "                    #pdist[:,0] = x\n",
        "                    #pdist[:,1] = x\n",
        "\n",
        "                    pdist = R_DTW(fake_data, real_data)\n",
        "             \n",
        "                    # Loss for D.\n",
        "                    d_loss = LeakyReLU(LossR - LossF + pdist).mean()\n",
        "                    d_loss.backward()\n",
        "\n",
        "                    #gp = get_direct_gradient_penalty(discriminator, real_data, 10, True)\n",
        "                    #gp.backward()\n",
        "\n",
        "                    # Gradient of D.\n",
        "                    gradD = real_data.grad\n",
        "\n",
        "                    d_optimizer.step() \n",
        "\n",
        "                    # -----------------\n",
        "                    #  Train Generator\n",
        "                    # -----------------\n",
        "                    # Update G network, freeze D.\n",
        "                    for p in discriminator.parameters():\n",
        "                        p.requires_grad = False \n",
        "\n",
        "                    g_optimizer.zero_grad()\n",
        "                    h_g = generator.init_hidden()\n",
        "\n",
        "                    # Sample noise as generator input\n",
        "                    noise_sample = Variable(noise(len(sample_data), params['seq_len'])).to(device)\n",
        "\n",
        "                    # Generate a batch of fake data\n",
        "                    fake_data = generator.forward(noise_sample,h_g)\n",
        "                    fake_data = fake_data.view(params['batch_size'], -1, params['seq_len'], 1)\n",
        "\n",
        "                    # Loss F for fake.\n",
        "                    LossF = discriminator(fake_data)\n",
        "                    # Loss measures generator's ability to fool the discriminator\n",
        "                    g_loss = LossF.mean()\n",
        "\n",
        "                    g_loss.backward()\n",
        "                    g_optimizer.step() \n",
        "\n",
        "\n",
        "                if n_batch == (num_batches - 2):\n",
        "                    G_losses.append(g_loss.item())\n",
        "                    D_losses.append(d_loss.item())\n",
        "                  \n",
        "                    torch.save(generator.state_dict(), savepath+'/MBD_' +str(mbd)+ '/gen/generator_state_'+str(n)+'.pt')\n",
        "                    torch.save(discriminator.state_dict(), savepath+'/MBD_' +str(mbd)+ '/disc/discriminator_state_'+str(n)+'.pt')\n",
        "                                   \n",
        "        # Dumping the errors for each training epoch.\n",
        "        with open(savepath+'/MBD_'+str(mbd)+'/generator_losses.txt', 'wb') as fp:\n",
        "            pickle.dump(G_losses, fp)\n",
        "        with open(savepath+'/MBD_' +str(mbd)+ '/discriminator_losses.txt', 'wb') as fp:\n",
        "            pickle.dump(D_losses, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooFkDiwNPsti"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXYoRDbJV2ce"
      },
      "source": [
        "# Initialise Parameters\n",
        "parameters = init_params(datafile)\n",
        "# Choose GAN function to use\n",
        "LS_GAN(filename=datapath+datafile, params=parameters, savepath=savepath)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}